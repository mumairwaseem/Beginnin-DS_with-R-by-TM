---
title: "MachineLearningSupervised"
author: "Umair Waseem"
date: "2022-10-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Machine Learning Supervised

Machine learning concerns learning from data. You do not explicitly develop an algorithm for solving a particular problem. You use a generic algorithm and let it learn how to solve the problem from those.


* ## Supervised Learning
  Supervised learning is used when you have variables you want to predict using other variables--situations like linear regression where you have some input variables for example x and you want a model that predicts output variables y =f(x)
  
  
## Logistic Regression (Classification)
  
  Here We'll use the breast cancer data from the mlbench library.
  Ask if the clump thickness has an effect on the risk of a tumor being malignant. That is we want to see if we can predict the _Class_ variable from C1. 
  
```{r}
library(mlbench)
library(dplyr)
library(tidyverse)
data("BreastCancer")
BreastCancer |> head()


```
```{r}
BreastCancer |> 
  ggplot (aes(x=Cl.thickness, y = Class)) +
  geom_jitter(height = 0.05, width = 0.3, alpha = 0.4)



```
  
  For classification, We still specify the prediction function y=f(x) using the formula y`x is just binary now. To fi a logistic regression, we need to use the _glm()_ function (generalized linear model) with family set to "binomial". 
  We cannot fit the breast cancer data with logistic regression. There are two problems:-
* The breast cancer data set considers the clump thickness order factor, but for logistic regression, we need the input variable to be numeric.. Generally it is not advised to directly translate categorical data into numeric data, but here for this data we can. 
* The _glm()_ function expects the response variable to be numerical, coding the classes like 0 or 1, while the breast cancer data again encodes the classes as a factor. It little varies from algorithm to algorithm whether a factor or a numerical encoding is expected for classification. 
  
  
```{r}
BreastCancer |>
  mutate(Thickness = 
           as.numeric(as.character(Cl.thickness))) |>
  mutate(Malignant = ifelse(Class != "benign", 1, 0)) |>
  ggplot(aes(x = Thickness, y = Malignant)) +
  geom_jitter(height = 0.05, width = 0.3, alpha = 0.4) +
  geom_smooth(method ="glm", formula = y~x, 
              method.args = list(family = "binomial"))
          
  






```
  
```{r}

BreastCancer |>
  mutate(Thickness = 
           as.numeric(as.character(Cl.thickness))) %>%
  mutate(Malignant = ifelse(Class != "benign", 1, 0)) %>%
  glm(Malignant ~ Thickness,
      family = "binomial",
      data = .)
  
# glm(response ~ predictor, family = "binomial", data)
# The glm() function returns a model object, therefore we may apply extractor functions, such as summary(), fitted(), or predict() among others. Please not that output numbers are on the logit scale. To actually predict probabilities we need to provided the predict() function an additional argument _type = "response"_


```
  ## Evaluating Classification Models
  
In classification you want to know how many data points are classified correctly and how many are not. 
As an example here is breast cancer data and fit a model:

```{r}

formatted_data <- BreastCancer |>
  mutate(Thickness = as.numeric(as.character(Cl.thickness)), 
         Cell.size = 
           as.numeric(as.character(Cell.size))) %>%
  mutate(Malignant = ifelse(Class != "benign", 1, 0))
                           

```

  